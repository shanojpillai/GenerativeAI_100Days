{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4KNWsVf7MksHMz7ziXxBf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanojpillai/GenerativeAI_100Days/blob/main/Day_3_BagOfWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "u734bl0zIHce"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data (stopwords and punkt for tokenization)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTVO_kw_LNcM",
        "outputId": "f0d2b165-858f-46a0-d7da-47fef2c50684"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sample text data\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The fox is quick and the dog is lazy.\",\n",
        "    \"The dog is brown and lazy.\"\n",
        "]"
      ],
      "metadata": {
        "id": "-gK8-mHtNZAM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QhjwdKJdNsVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize, remove stopwords, and count word frequencies\n",
        "processed_texts = []\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    processed_texts.append(tokens)\n",
        "\n",
        "# Flatten the list of tokens and count word frequencies\n",
        "all_words = [word for tokens in processed_texts for word in tokens]\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Convert the word counts to a DataFrame for easy viewing\n",
        "df_bow = pd.DataFrame([word_counts])\n",
        "print(\"Bag-of-Words Matrix:\")\n",
        "print(df_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Z7-f8dNeSF",
        "outputId": "8da2a023-2b68-4af1-a5ac-d661aa0b8bf5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Matrix:\n",
            "   quick  brown  fox  jumps  lazy  dog\n",
            "0      2      2    2      1     3    3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = ['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
        " 'ok lar joke wif u oni',\n",
        " 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli',\n",
        " 'u dun say earli hor u c alreadi say',\n",
        " 'nah think goe usf live around though',\n",
        " 'freemsg hey darl week word back like fun still tb ok xxx std chg send rcv',\n",
        " 'even brother like speak treat like aid patent',\n",
        " 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press copi friend callertun',\n",
        " 'winner valu network custom select receivea prize reward claim call claim code kl valid hour',\n",
        " 'mobil month u r entitl updat latest colour mobil camera free call mobil updat co free',\n",
        " 'gonna home soon want talk stuff anymor tonight k cri enough today',\n",
        " 'six chanc win cash pound txt csh send cost p day day tsandc appli repli hl info',\n",
        " 'urgent week free membership prize jackpot txt word claim c www dbuk net lccltd pobox ldnw rw',\n",
        " 'search right word thank breather promis wont take help grant fulfil promis wonder bless time',\n",
        " 'date sunday',\n",
        " 'xxxmobilemovieclub use credit click wap link next txt messag click http wap xxxmobilemovieclub com n qjkgighjjgcbl',\n",
        " 'oh k watch',\n",
        " 'eh u rememb spell name ye v naughti make v wet',\n",
        " 'fine way u feel way gota b',\n",
        " 'england v macedonia dont miss goal team news txt ur nation team eg england tri wale scotland txt poboxox w wq',\n",
        " 'serious spell name',\n",
        " 'go tri month ha ha joke',\n",
        " 'pay first lar da stock comin',\n",
        " 'aft finish lunch go str lor ard smth lor u finish ur lunch alreadi',\n",
        " 'ffffffffff alright way meet sooner',\n",
        " 'forc eat slice realli hungri tho suck mark get worri know sick turn pizza lol',\n",
        " 'lol alway convinc',\n",
        " 'catch bu fri egg make tea eat mom left dinner feel love',\n",
        " 'back amp pack car let know room',\n",
        " 'ahhh work vagu rememb feel like lol',\n",
        " 'wait still clear sure sarcast x want live us',\n",
        " 'yeah got v apologet n fallen actin like spoilt child got caught till go badli cheer',\n",
        " 'k tell anyth',\n",
        " 'fear faint housework quick cuppa',\n",
        " 'thank subscript rington uk mobil charg month pleas confirm repli ye repli charg',\n",
        " 'yup ok go home look time msg xuhui go learn nd may lesson',\n",
        " 'oop let know roommat done',\n",
        " 'see letter b car',\n",
        " 'anyth lor u decid',\n",
        " 'hello saturday go text see decid anyth tomo tri invit anyth',\n",
        " 'pl go ahead watt want sure great weekend abiola',\n",
        " 'forget tell want need crave love sweet arabian steed mmmmmm yummi',\n",
        " 'rodger burn msg tri call repli sm free nokia mobil free camcord pleas call deliveri tomorrow',\n",
        " 'see',\n",
        " 'great hope like man well endow lt gt inch',\n",
        " 'call messag miss call',\n",
        " 'get hep b immunis nigeria']"
      ],
      "metadata": {
        "id": "_zBJ6wiRNxjG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize, remove stopwords, and count word frequencies\n",
        "processed_texts = []\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    processed_texts.append(tokens)\n",
        "\n",
        "# Flatten the list of tokens and count word frequencies\n",
        "all_words = [word for tokens in processed_texts for word in tokens]\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Convert the word counts to a DataFrame for easy viewing\n",
        "df_bow = pd.DataFrame([word_counts])\n",
        "print(\"Bag-of-Words Matrix:\")\n",
        "print(df_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KSOiEHaN2Pi",
        "outputId": "a32d46e1-7b6a-41c7-ac97-61277f13e23e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Matrix:\n",
            "   go  jurong  point  crazi  avail  bugi  n  great  world  la  ...  hope  man  \\\n",
            "0   8       1      1      1      1     1  3      3      1   1  ...     1    1   \n",
            "\n",
            "   well  endow  lt  gt  inch  hep  immunis  nigeria  \n",
            "0     1      1   1   1     1    1        1        1  \n",
            "\n",
            "[1 rows x 330 columns]\n"
          ]
        }
      ]
    }
  ]
}