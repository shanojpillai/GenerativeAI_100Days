{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOalJYeRS2d6pKI5GzG56qF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanojpillai/GenerativeAI_100Days/blob/main/Detailed_Explanation_of_Bag_of_Words_Model_and_N_gram_Feature_Extraction_in_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Bag of Words (BoW)**\n",
        "The **Bag of Words (BoW)** model is a text representation method used in Natural Language Processing (NLP). It’s a way of encoding text data that transforms documents (sentences, paragraphs, or entire texts) into a numerical format, making them usable for machine learning models. Key points of the Bag of Words model include:\n",
        "\n",
        "**Tokenization:** Splitting text into individual words or terms.\n",
        "\n",
        "**Ignoring Grammar and Order:** It only considers whether a term appears in the\n",
        "document, ignoring its position or context.\n",
        "**Frequency-Based Representation:** Each unique word or term becomes a feature in a feature matrix, and each document is represented by a row in this matrix.\n",
        "\n",
        "For example, given the sentence: \"This is the first document,\" the Bag of Words model will treat each unique word as a feature and count how often it appears.\n",
        "\n",
        "This model is beneficial because it’s simple and effective, especially for basic NLP tasks like text classification and sentiment analysis. However, it loses information about the word order, which sometimes limits its ability to capture context.\n",
        "\n",
        "**2. CountVectorizer**\n",
        "The CountVectorizer class in Scikit-Learn automates the Bag of Words process. Here’s how it works:\n",
        "\n",
        "**Building a Vocabulary:** When fitting a CountVectorizer to a corpus, it creates a vocabulary dictionary. Each unique word in the corpus is mapped to an index.\n",
        "\n",
        "**Transforming Text Data:** The text data is then transformed into a matrix where each row corresponds to a document and each column represents a unique word in the vocabulary. The values in the matrix represent either the count or the presence of words.\n",
        "\n",
        "**Key Parameters of CountVectorizer**\n",
        "**max_features**: Limits the vocabulary size to the max_features most common words. This can be helpful when dealing with large text corpora.\n",
        "**binary:** When set to True, this encodes each feature as binary, indicating only the presence (1) or absence (0) of a word in a document, rather than its frequency. Binary encoding is helpful when the importance of a word does not depend on its frequency.\n",
        "**ngram_range:** Defines the range of n-grams (combinations of consecutive words) to be considered. The default setting (1, 1) considers only single words (unigrams). Setting ngram_range=(2, 3) allows the model to capture pairs of words (bigrams) and triples of words (trigrams).\n",
        "\n",
        "**3. Corpus**\n",
        "The corpus is simply a collection of text documents. Each document in the corpus can be a sentence, paragraph, or longer text. In this notebook, the corpus consists of four simple sentences:\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "This corpus will be transformed into a numerical matrix representation using CountVectorizer.\n",
        "\n",
        "**4. Vocabulary and Feature Matrix**\n",
        "**Vocabulary**\n",
        "When the CountVectorizer is fitted to the corpus, it generates a vocabulary dictionary. Each word in the corpus is assigned a unique index, which becomes a feature in the matrix. For example, in the vocabulary:\n",
        "\n",
        "\n",
        "`{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}`\n",
        "Each word corresponds to a unique integer, which represents its column position in the matrix. This mapping allows the text data to be transformed into a structured format.\n",
        "\n",
        "**Feature Matrix**\n",
        "The feature matrix (also called the document-term matrix) is a representation of the corpus in numerical form. Each row represents a document, and each column represents a unique word from the vocabulary. The values indicate the count or presence of words, depending on the encoding type.\n",
        "\n",
        "For example, a feature matrix might look like this in binary encoding:\n",
        "\n",
        "\n",
        "array([\n",
        "    [0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
        "    [0, 1, 0, 1, 0, 1, 1, 0, 1],\n",
        "    [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
        "    [0, 1, 1, 1, 0, 0, 1, 0, 1]\n",
        "])\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "Each row corresponds to a document in the corpus, and each column corresponds to a word. The binary encoding represents whether each word is present (1) or absent (0).\n",
        "\n",
        "**5. N-grams**\n",
        "An n-gram is a contiguous sequence of n items from a given text. In the context of NLP, an n-gram usually refers to a sequence of n words.\n",
        "\n",
        "Unigrams: Single words, which are typically represented by ngram_range=(1, 1).\n",
        "Bigrams: Sequences of two words, represented by ngram_range=(2, 2).\n",
        "Trigrams: Sequences of three words, represented by ngram_range=(3, 3).\n",
        "In this notebook, CountVectorizer is initialized with ngram_range=(2, 3), capturing both bigrams and trigrams. This allows the model to consider sequences of two or three words, preserving some context that single words alone cannot capture. This results in a new vocabulary of word pairs and triplets, which helps in understanding phrases or multi-word expressions.\n",
        "\n",
        "For example:\n",
        "\n",
        "plaintext\n",
        "Copy code\n",
        "{'this is': 21, 'is the': 5, 'the first': 12, 'first document': 4, ...}\n",
        "This vocabulary now includes phrases like \"this is\" or \"first document,\" and the matrix represents whether these phrases appear in each document.\n",
        "\n",
        "**6. Binary Encoding**\n",
        "Binary encoding is a way to represent the presence of words or n-grams in documents without considering their frequency. In this mode, each entry in the feature matrix is 1 if a word (or n-gram) is present in the document and 0 if it is absent.\n",
        "\n",
        "This approach is often used when the mere presence of a word is more informative than its frequency, especially for tasks like text classification, where repeating words might not add much value.\n",
        "\n",
        "Summary\n",
        "Bag of Words: Converts text into a structured numerical format by creating a feature for each unique word or n-gram.\n",
        "CountVectorizer: Automates the process of creating a vocabulary and transforming the text into a matrix.\n",
        "Corpus: A collection of text documents to analyze.\n",
        "Vocabulary: Maps words to integer indices, representing their position in the feature matrix.\n",
        "Feature Matrix: Represents documents as rows and words as columns, showing either the count or presence of each word.\n",
        "N-grams: Sequences of n consecutive words, capturing phrases and adding contextual information.\n",
        "Binary Encoding: Indicates the presence (1) or absence (0) of words in each document without counting their frequency."
      ],
      "metadata": {
        "id": "MUMx2i9pvOw0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjuUE1KcwTmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}